{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4",
   "authorship_tag": "ABX9TyNpRhnqvCO5CbtzcNVgb7cv",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fibonacci-2/masterout/blob/main/masterout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45ykJe8vXw61",
    "outputId": "586da6cc-8873-464c-c403-1de0f7b4b6fb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "RoBERTa base model for sequence classification loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install the transformers library",
    "!pip install transformers",
    "",
    "# Import necessary libraries",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer",
    "import torch",
    "from google.colab import drive",
    "drive.mount('/content/drive')",
    "",
    "# Specify the path to your model checkpoints in Google Drive",
    "model_path = '/content/drive/MyDrive/masterout/depression_roberta-base/checkpoint-20094'",
    "",
    "# Load the model and tokenizer for sequence classification",
    "try:",
    " model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)",
    " tokenizer = RobertaTokenizer.from_pretrained(model_path)",
    " print(\"RoBERTa base model for sequence classification loaded successfully.\")",
    "except Exception as e:",
    " print(f\"An error occurred while loading the model or tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd",
    "from sklearn.model_selection import train_test_split",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments",
    "import torch",
    "import os",
    "",
    "# Load the dataset",
    "df = pd.read_csv('/content/drive/My Drive/amhd-copy/data/cleaned_arabic_datasetv2.csv')",
    "",
    "# Filter for 'ocd' and 'control' classes",
    "df_binary = df[df['condition'].isin(['depression', 'control'])].copy()",
    "",
    "# Balance the dataset",
    "ocd_count = df_binary[df_binary['condition'] == 'depression'].shape[0]",
    "control_df = df_binary[df_binary['condition'] == 'control']",
    "sampled_control_df = control_df.sample(n=ocd_count, random_state=42)",
    "",
    "balanced_df = pd.concat([df_binary[df_binary['condition'] == 'depression'], sampled_control_df])",
    "",
    "# Shuffle the balanced dataset",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)",
    "",
    "",
    "# Map 'ocd' to 1 and 'control' to 0",
    "balanced_df['label'] = balanced_df['condition'].apply(lambda x: 1 if x == 'ocd' else 0)",
    "",
    "# Handle potential missing values in 'selftext' and 'title'",
    "balanced_df['selftext'] = balanced_df['selftext'].fillna('')",
    "balanced_df['title'] = balanced_df['title'].fillna('')",
    "",
    "# Combine 'title' and 'selftext' for classification",
    "balanced_df['text'] = balanced_df['title'] + \" \" + balanced_df['selftext']",
    "",
    "# Split the dataset into training and validation sets",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, random_state=42)",
    "",
    "# Use the tokenizer loaded from the previous cell",
    "# tokenizer is already loaded in cell 45ykJe8vXw61",
    "",
    "# Create a custom dataset class",
    "class OCDDataset(torch.utils.data.Dataset):",
    " def __init__(self, encodings, labels):",
    " self.encodings = encodings",
    " self.labels = labels",
    "",
    " def __getitem__(self, idx):",
    " item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}",
    " item['labels'] = torch.tensor(self.labels[idx])",
    " return item",
    "",
    " def __len__(self):",
    " return len(self.labels)",
    "",
    "# Tokenize the text data using the tokenizer from the previous cell",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=512)",
    "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True, max_length=512)",
    "",
    "# Create dataset objects",
    "train_dataset = OCDDataset(train_encodings, list(train_df['label']))",
    "val_dataset = OCDDataset(val_encodings, list(val_df['label']))",
    "",
    "print(\"Binary classification setup complete. train_dataset and val_dataset are ready.\")",
    "print(f\"Training dataset size: {len(train_dataset)}\")",
    "print(f\"Validation dataset size: {len(val_dataset)}\")",
    "",
    "# Load the RoBERTa model for sequence classification",
    "# Use the base model loaded in the previous cell and adapt it for sequence classification",
    "# model = RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/masterout/ptsd_roberta-base', num_labels=2)",
    "",
    "",
    "# Define training arguments",
    "training_args = TrainingArguments(",
    " output_dir='./results', # output directory",
    " num_train_epochs=3, # number of training epochs",
    " per_device_train_batch_size=16, # batch size per device during training",
    " per_device_eval_batch_size=64, # batch size for evaluation",
    " warmup_steps=500, # number of warmup steps for learning rate scheduler",
    " weight_decay=0.01, # strength of weight decay",
    " logging_dir='./logs', # directory for storing logs",
    " logging_steps=10,",
    " eval_strategy=\"epoch\"",
    ")",
    "",
    "# Create Trainer instance",
    "trainer = Trainer(",
    " model=model, # the instantiated Transformers model to be trained",
    " args=training_args, # training arguments, defined above",
    " train_dataset=train_dataset, # training dataset",
    " eval_dataset=val_dataset # evaluation dataset",
    ")",
    "",
    "print(\"Trainer instance created. Ready to train the model.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foxpDcNZZ7hm",
    "outputId": "671d988b-579a-4664-abf1-5f99cf1b0fdd"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Binary classification setup complete. train_dataset and val_dataset are ready.\n",
      "Training dataset size: 33736\n",
      "Validation dataset size: 8434\n",
      "Trainer instance created. Ready to train the model.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "zCaK739Rf8V_",
    "outputId": "71691225-e805-4263-c93d-0082c68852dc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='6327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 598/6327 06:14 < 1:00:00, 1.59 it/s, Epoch 0.28/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ]
  }
 ]
}