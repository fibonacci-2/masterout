{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNpRhnqvCO5CbtzcNVgb7cv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fibonacci-2/masterout/blob/main/masterout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ykJe8vXw61",
        "outputId": "586da6cc-8873-464c-c403-1de0f7b4b6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "RoBERTa base model for sequence classification loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your model checkpoints in Google Drive\n",
        "model_path = '/content/drive/MyDrive/masterout/depression_roberta-base/checkpoint-20094'\n",
        "\n",
        "# Load the model and tokenizer for sequence classification\n",
        "try:\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
        "    print(\"RoBERTa base model for sequence classification loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model or tokenizer: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/amhd-copy/data/cleaned_arabic_datasetv2.csv')\n",
        "\n",
        "# Filter for 'ocd' and 'control' classes\n",
        "df_binary = df[df['condition'].isin(['depression', 'control'])].copy()\n",
        "\n",
        "# Balance the dataset\n",
        "ocd_count = df_binary[df_binary['condition'] == 'depression'].shape[0]\n",
        "control_df = df_binary[df_binary['condition'] == 'control']\n",
        "sampled_control_df = control_df.sample(n=ocd_count, random_state=42)\n",
        "\n",
        "balanced_df = pd.concat([df_binary[df_binary['condition'] == 'depression'], sampled_control_df])\n",
        "\n",
        "# Shuffle the balanced dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Map 'ocd' to 1 and 'control' to 0\n",
        "balanced_df['label'] = balanced_df['condition'].apply(lambda x: 1 if x == 'ocd' else 0)\n",
        "\n",
        "# Handle potential missing values in 'selftext' and 'title'\n",
        "balanced_df['selftext'] = balanced_df['selftext'].fillna('')\n",
        "balanced_df['title'] = balanced_df['title'].fillna('')\n",
        "\n",
        "# Combine 'title' and 'selftext' for classification\n",
        "balanced_df['text'] = balanced_df['title'] + \" \" + balanced_df['selftext']\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use the tokenizer loaded from the previous cell\n",
        "# tokenizer is already loaded in cell 45ykJe8vXw61\n",
        "\n",
        "# Create a custom dataset class\n",
        "class OCDDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Tokenize the text data using the tokenizer from the previous cell\n",
        "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = OCDDataset(train_encodings, list(train_df['label']))\n",
        "val_dataset = OCDDataset(val_encodings, list(val_df['label']))\n",
        "\n",
        "print(\"Binary classification setup complete. train_dataset and val_dataset are ready.\")\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Load the RoBERTa model for sequence classification\n",
        "# Use the base model loaded in the previous cell and adapt it for sequence classification\n",
        "# model = RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/masterout/ptsd_roberta-base', num_labels=2)\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "print(\"Trainer instance created. Ready to train the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foxpDcNZZ7hm",
        "outputId": "671d988b-579a-4664-abf1-5f99cf1b0fdd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary classification setup complete. train_dataset and val_dataset are ready.\n",
            "Training dataset size: 33736\n",
            "Validation dataset size: 8434\n",
            "Trainer instance created. Ready to train the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "zCaK739Rf8V_",
        "outputId": "71691225-e805-4263-c93d-0082c68852dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='598' max='6327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 598/6327 06:14 < 1:00:00, 1.59 it/s, Epoch 0.28/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}